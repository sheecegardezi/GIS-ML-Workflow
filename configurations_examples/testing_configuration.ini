[Workflow]
# switches to control which step to preform {True, False}
DataPreparation = True
DataExploration = False
FeatureExtraction = True
HyperParameterOptimization = True
ModelExploration = True
PredictionMapping = False
CovariateDrift = True

[OutputFolder]
# required, output folder to save results {Linux Style Absolut Path}
/g/data/ge3/testing

[Model]
# required, name of modeling function {XGBRegressor, SVMRegressor, CatBoostRegressor, LightGBMRegressor, RandomForestRegressor, BoostedXGBRegressor, BoostedSVMRegressor}
model_function = XGBRegressor
# dictionary of parameters for the model selected
parameters = {'max_depth': 3, 'learning_rate': 10, 'num_boost_round': 10 }

[Target]
# required, target property to predict
target_property = depth_cen
# experimental not implemented in workflow
# target_weight = weights
target_groupcv = weights
target_groupcv_n_splits = 2
# required, linux style absolut path to vector file
target_path = /g/data/ge3/data/targets/cenozoic_depth/Base_cenozoic_albers_JV.shp
# required, percentage of target dataset to be used for out of sample testing
percentage_oos = 10
# or path to out of sample dataset
# oos_path = /abs/path/to/xyz.shp

[Covariates]
# required, list of linux style absolut path covariates in geotiff format
/g/data/ge3/data/covariates/water-85m_1.tif
/g/data/ge3/data/covariates/water-85m_2.tif
/g/data/ge3/data/covariates/water-85m_3.tif
/g/data/ge3/data/covariates/tpi_300.tif

[FeatureExtraction]
# required, name of feature extraction algorithm to use {FeatureRankingByRandomness, FeatureRankingByEliminationCV, FeatureRankingByEliminationOOS, FeatureRankingByShap, FeatureRankingByGroupCV}
algorithm = FeatureRankingByGroupCV
# required, number of highest ranking features to use, enter -1 to use all the features
no_features_to_select = -1
# required, which scoring function to use for evaluation {mean_squared_error_scorer, mean_absolute_error_scorer, r2_scorer, rmse_scorer, adjusted_r2_scorer}
scoring_function = adjusted_r2_scorer

[HyperParameterOptimization]
# required, provided name of algorithm to use for hyperparameter optimization {BayesianOptimization, GridSearch, HyperOptSearch}
algorithm = HyperOptSearch
# required, provide dictionary of modeling function specific parameters to optimise
hyper_parameters = { "num_boost_round": loguniform(100, 10000), "max_depth": uniform(3, 15), "subsample": quniform(0.25, 0.75, 0.01), "colsample_bytree": quniform(0.05, 0.5, 0.01), "colsample_bylevel": quniform(0.05, 0.5, 0.01), "learning_rate": uniform(0.001, 0.1),"min_child_weight": choice([1, 2, 3, 4, 5, 6]),"gamma": uniform(0.1, 0.2), "max_delta_step": uniform(1, 10)}
# required, number of searches Hyper Parameter Optimization algorithms can do to find optimal parameters
n_iteration = 3
# required, the scoring function which will be used by HPO algorithm to selected optimal parameters
scoring_functions = [mean_squared_error_scorer, mean_absolute_error_scorer, r2_scorer, rmse_scorer]
# scoring_function_to_use_for_evaluation can be any of the following values, adding cv_  or oos_  or groupcv_ tag to any of the scoring function:
# groupcv_mean_squared_error_scorer, groupcv_mean_absolute_error_scorer, groupcv_r2_scorer, groupcv_rmse_scorer
# cv_mean_squared_error_scorer, cv_mean_absolute_error_scorer, cv_r2_scorer, cv_rmse_scorer
# oos_mean_squared_error_scorer, oos_mean_absolute_error_scorer, oos_r2_scorer, oos_rmse_scorer
scoring_function_to_use_for_evaluation = groupcv_r2_scorer
# required, number of splits for cross validation
n_splits = 2

[PredictionMapping]
# required, linux style absolut path for mask area of interest to preform prediction for
area_of_interest = /g/data/ge3/data/mask/area_of_interest.tif
# required, linux style absolut path for trained model to use for making prediction
path_to_trained_model = None

[ModelExploration]
# required, number of splits for cross validation
n_splits = 2
# required, linux style absolut path for trained model
path_to_trained_model = None
# required, list of scoring functions for which the modeling function scores will be reported
scoring_functions = [mean_squared_error_scorer, mean_absolute_error_scorer, r2_scorer, rmse_scorer]
# not required, choose value from {True, False}, it will create a covariate drift map
covariate_drift =  True

[CovariateDrift]
# required, choose an algorithm from: LogisticRegression, RandomForestClassifier, GaussianNB, KNeighborsClassifier
modeling_function = KNeighborsClassifier


[Control]
# required, number of cpus to be used by each task
cpus_per_job = 8
# required, number of gpus to be used by each task
gpu_per_job = 0

[Intermediate]
# automatically generated after running DataPreparation, edit to provide custom value
training_dataset = None
# automatically generated after running DataPreparation, edit to provide custom value
oos_dataset = None
# automatically generated after running DataPreparation, edit to provide custom value
selected_features = None
# automatically generated after running DataPreparation, edit to provide custom value
covariates = None
# automatically generated after running DataPreparation, edit to provide custom value
area_of_interest = None
# automatically generated after running DataPreparation, edit to provide custom value
training_drift_dataset = None
# automatically generated after running DataPreparation, edit to provide custom value
drift_dataset = None
# automatically generated after running DataPreparation, edit to provide custom value
drift_vector_dataset = None
# automatically generated after running DataPreparation, edit to provide custom value
path_to_predicted_drift_geotiff = None

[Results]
# automatically generated linux style absolut path after running FeatureExtraction
path_to_feature_ranking_results = None
# automatically generated json after running HyperParameterOptimization
best_estimator_prams = None
# automatically generated linux style absolut path after running HyperParameterOptimization
best_path_to_trained_model = None
# automatically generated linux style absolut path after running HyperParameterOptimization
path_to_hyper_parameter_search_results = None
# automatically generated json after running HyperParameterOptimization
best_estimator_scores = None
# automatically generated linux style absolut path after running PredictionMapping
path_to_predicted_geotiff = None
